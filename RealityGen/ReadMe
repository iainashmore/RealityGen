https://github.com/MochiDiffusion/MochiDiffusion/wiki/How-to-convert-Stable-Diffusion-models-to-Core-ML

cd /Users/iainashmore/Projects/ml-stable-diffusion/model

conda activate coreml_stable_diffusion

python convert_original_stable_diffusion_to_diffusers.py --checkpoint_path Public-Prompts-Pixel-Model.ckpt --device cpu --extract_ema --dump_path Public-Prompts-Pixel-Model_diffusers

python -m python_coreml_stable_diffusion.torch2coreml --latent-w 64 --latent-h 64 --compute-unit CPU_AND_GPU --convert-vae-decoder --convert-vae-encoder --convert-unet --unet-support-controlnet --convert-text-encoder --model-version <MODEL-NAME>_diffusers --bundle-resources-for-swift-cli --attention-implementation ORIGINAL -o <MODEL-NAME>_original_512x768 && python -m python_coreml_stable_diffusion.torch2coreml --latent-w 64 --latent-h 96 --compute-unit CPU_AND_GPU --convert-unet --model-version <MODEL-NAME>_diffusers --bundle-resources-for-swift-cli --attention-implementation ORIGINAL -o <MODEL-NAME>_original_64x64


python -m python_coreml_stable_diffusion.torch2coreml --convert-vae-decoder --convert-vae-encoder --convert-unet --unet-support-controlnet --convert-text-encoder --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation SPLIT_EINSUM_V2 -o Public-Prompts-Pixel-Model_split-einsum-v2 && python -m python_coreml_stable_diffusion.torch2coreml --convert-unet --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation SPLIT_EINSUM_V2 -o Public-Prompts-Pixel-Model_split-einsum-v2

python -m python_coreml_stable_diffusion.torch2coreml --latent-w 64 --latent-h 64 --convert-vae-decoder --convert-vae-encoder --convert-unet --unet-support-controlnet --convert-text-encoder --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation SPLIT_EINSUM_V2 -o Public-Prompts-Pixel-Model_split-einsum-64v2 && python -m python_coreml_stable_diffusion.torch2coreml --latent-w 64 --latent-h 64  --convert-unet --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation SPLIT_EINSUM_V2 -o Public-Prompts-Pixel-Model_split-einsum-64v2

python -m python_coreml_stable_diffusion.torch2coreml --latent-w 96 --latent-h 96 --convert-vae-decoder --convert-vae-encoder --convert-unet --unet-support-controlnet --convert-text-encoder --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation SPLIT_EINSUM_V2 -o Public-Prompts-Pixel-Model_split-einsum-96v2 && python -m python_coreml_stable_diffusion.torch2coreml --latent-w 96 --latent-h 96  --convert-unet --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation SPLIT_EINSUM_V2 -o Public-Prompts-Pixel-Model_split-einsum-96v2

python -m python_coreml_stable_diffusion.torch2coreml --latent-w 64 --latent-h 64 --compute-unit CPU_AND_GPU --convert-vae-decoder --convert-vae-encoder --convert-unet --unet-support-controlnet --convert-text-encoder --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation ORIGINAL -o Public-Prompts-Pixel-Model_original_64x64 && python -m python_coreml_stable_diffusion.torch2coreml --latent-w 64 --latent-h 64 --compute-unit CPU_AND_GPU --convert-unet --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation ORIGINAL -o Public-Prompts-Pixel-Model_original_64x64


python -m python_coreml_stable_diffusion.torch2coreml --latent-w 128 --latent-h 128 --compute-unit CPU_AND_GPU --convert-vae-decoder --convert-vae-encoder --convert-unet --unet-support-controlnet --convert-text-encoder --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation ORIGINAL -o Public-Prompts-Pixel-Model_original_128x128 && python -m python_coreml_stable_diffusion.torch2coreml --latent-w 128 --latent-h 128 --compute-unit CPU_AND_GPU --convert-unet --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation ORIGINAL -o Public-Prompts-Pixel-Model_original_128x128

python -m python_coreml_stable_diffusion.torch2coreml --latent-w 128 --latent-h 128 --compute-unit CPU_AND_GPU --convert-vae-decoder --convert-vae-encoder --convert-unet --unet-support-controlnet --convert-text-encoder --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation ORIGINAL -o Public-Prompts-Pixel-Model_original_128x128 && python -m python_coreml_stable_diffusion.torch2coreml --latent-w 128 --latent-h 128 --compute-unit CPU_AND_GPU --convert-unet --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation ORIGINAL -o Public-Prompts-Pixel-Model_original_128x128

python -m python_coreml_stable_diffusion.torch2coreml --quantize-nbits 2 --compute-unit CPU_AND_GPU --convert-vae-decoder --convert-vae-encoder --convert-unet --unet-support-controlnet --convert-text-encoder --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation ORIGINAL -o Public-Prompts-Pixel-Model_original_128x128_2 && python -m python_coreml_stable_diffusion.torch2coreml --quantize-nbits 2 --compute-unit CPU_AND_GPU --convert-unet --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation ORIGINAL -o Public-Prompts-Pixel-Model_original_128x128_2

python -m python_coreml_stable_diffusion.torch2coreml --quantize-nbits 4 --compute-unit CPU_AND_GPU --convert-vae-decoder --convert-vae-encoder --convert-unet --unet-support-controlnet --convert-text-encoder --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation ORIGINAL -o Public-Prompts-Pixel-Model_original_128x128_4 && python -m python_coreml_stable_diffusion.torch2coreml --quantize-nbits 4 --compute-unit CPU_AND_GPU --convert-unet --model-version Public-Prompts-Pixel-Model_diffusers --bundle-resources-for-swift-cli --attention-implementation ORIGINAL -o Public-Prompts-Pixel-Model_original_128x128_4

prompt: "Godzilla, full body game asset, in pixelsprite style"
negativePrompt: "out of frame"

Suggested CFG scale: 10
Sampler DDIM, steps: 40


prompts of the example images:
* cute cat full body, in pixelsprite style
* tarzan, standing, full character, in pixelsprite style
* magic potion, game asset, in pixelsprite style
* morpheus from the matrix character, standing full character, in pixelsprite style
* chair, in pixelsprite style
* barrel, game asset, in pixelsprite style
* godzilla, in pixelsprite style
__________________________

* isometric living room, detailed, in 16bitscene style
* dark arcade room, pink neon lights, detailed, in 16bitscene style
* living room, detailed, in 16bitscene style
* bathroom, in 16bitscene style
_________________________

* green landscape, tornado approaching, in 16bitscene style
* street in a sunny day. in 16bitscene style
* car driving away, synthwave outrun style wallpaper, in 16bitscene style
